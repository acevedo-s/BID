{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from transformers import AutoTokenizer\n",
    "from time import time \n",
    "### HYPERPARAMETERS\n",
    "LLM = 'Pythia'\n",
    "corpus = 'Wikitext'\n",
    "randomize = 0\n",
    "\n",
    "if LLM == 'OPT':\n",
    "  max_length = 401 # +1 for BOS token...\n",
    "  from transformers.models.opt import (\n",
    "                                      OPTModel, # no-head\n",
    "                                      OPTConfig,\n",
    "                                      )\n",
    "  modelname = \"facebook/opt-350m\"\n",
    "  config = OPTConfig.from_pretrained(modelname,\n",
    "                                  output_hidden_states=True,\n",
    "                                  )\n",
    "  model = OPTModel.from_pretrained(modelname,\n",
    "                                config=config,\n",
    "                                device_map=\"auto\",\n",
    "                                )\n",
    "elif LLM == 'Pythia':\n",
    "  max_length = 400 # No BOS token...\n",
    "  from transformers import (GPTNeoXForCausalLM,\n",
    "                            GPTNeoXConfig\n",
    "                            )\n",
    "  modelname = \"EleutherAI/pythia-410m-deduped\"\n",
    "  model = GPTNeoXForCausalLM.from_pretrained(\n",
    "                            modelname,\n",
    "                            revision=\"main\",\n",
    "                            # cache_dir=\"./pythia-410m-deduped/main\",\n",
    "                            )\n",
    "  config = GPTNeoXConfig.from_pretrained(modelname,\n",
    "                                        output_hidden_states=True,\n",
    "                                        )\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname,\n",
    "                                          padding_side='left',\n",
    "                                          device_map=\"auto\",\n",
    "                                          )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "print(vars(model),\n",
    "      file=open(f'{LLM}_model.txt','w'))\n",
    "  ###---\n",
    "\n",
    "\n",
    "wd = os.environ['WORK']\n",
    "path0 = \"/sacevedo/Data/Text/\"\n",
    "### TOKENS\n",
    "tokens_outputfolder0 = wd + path0 + f'{corpus}/{LLM}/input_tokens/'\n",
    "tokens_outputfolder = f'{tokens_outputfolder0}/max_length{max_length:d}/'\n",
    "if randomize:\n",
    "  tokens_outputfolder += 'randomize/'\n",
    "os.makedirs(tokens_outputfolder,exist_ok = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1801350/1801350 [00:04<00:00, 429702.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wt = load_dataset('wikitext','wikitext-103-raw-v1',split='train')\n",
    "wt = wt.filter(lambda x: len(x[\"text\"]) > 2*max_length) # removing white spacings and crap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 254768\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1801350/1801350 [00:04<00:00, 397041.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading took 11.0 sec\n",
      "i=0  As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \n",
      "\n",
      "len(texts)=254768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from time import time\n",
    "import numpy as np\n",
    "from utils import get_lengths\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", UserWarning)\n",
    "# warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "### DATASET + CHARACTER THRESHOLDING\n",
    "start = time()\n",
    "if corpus == 'Wikitext':\n",
    "  wt = load_dataset('wikitext','wikitext-103-raw-v1',split='train')\n",
    "  wt = wt.filter(lambda x: len(x[\"text\"]) > 2*max_length) # removing white spacings and crap\n",
    "  if False:\n",
    "    wt = wt.select(range(10000))\n",
    "  texts = wt['text']\n",
    "# elif corpus=='Tinystories':\n",
    "#   path0 = '/scratch/sacevedo/Tinystories'\n",
    "#   wt = load_dataset(path0,\n",
    "#                     data_files='TinyStoriesV2-GPT4-train.txt',\n",
    "#                     split='train[:15%]')\n",
    "#   wt = wt.filter(lambda x: x[\"text\"] != ' ' and x[\"text\"] != '')\n",
    "#   texts = wt['text'][0]\n",
    "#   for idx,element in enumerate(wt['text'][1:]):\n",
    "#     # if element == ' ': print('spaceeee')\n",
    "#     # if element == '': print('empty?')\n",
    "#     # texts.append(element)\n",
    "#     texts+=(element)\n",
    "#   texts = texts.split('<|endoftext|>')\n",
    "elif corpus == 'OWebtext':\n",
    "  wt = load_dataset('stas/openwebtext-10k','plain_text',split='train')\n",
    "  texts = wt['text']\n",
    "print(f'loading took {time()-start:.1f} sec')\n",
    "\n",
    "if True:\n",
    "  for i in range(1):\n",
    "    # print(texts[i])\n",
    "    print(f'{i=}',texts[i])\n",
    "print(f'{len(texts)=:d}')\n",
    "###---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###TOKENIZATION\n",
    "# start = time()\n",
    "# max_input_length = config.max_position_embeddings\n",
    "# # print(f'{config.max_position_embeddings=}')\n",
    "# # max_input_length = max_length\n",
    "# x = tokenizer(texts,\n",
    "#               return_tensors=\"pt\",\n",
    "#               padding=True,\n",
    "#               truncation=True, \n",
    "#               max_length=max_input_length,\n",
    "#               ).to(device)\n",
    "# del texts, wt\n",
    "# print(f'tokenization took {time()-start:.1f} seconds')\n",
    "# print(f'{x[\"input_ids\"].shape=}')\n",
    "# print(f'{x[\"attention_mask\"].shape=}')\n",
    "# ###---\n",
    "# print('--------------------------------------------------')\n",
    "# # print(f'{x[\"input_ids\"]=}')\n",
    "# # print(f'{x[\"attention_mask\"]=}')\n",
    "\n",
    "# ### MIN-TOKEN-LENGTH THRESHOLDING\n",
    "# print(f'MIN-TOKEN-LENGTH THRESHOLDING:')\n",
    "# Ns0,max_length_in_batch = x[\"input_ids\"].shape\n",
    "# numtokens = get_lengths(x['attention_mask'])\n",
    "# selected_ids = torch.where(numtokens>max_length,1,0)\n",
    "# x['input_ids'] = x['input_ids'][selected_ids.nonzero(as_tuple=True)]\n",
    "# x['attention_mask'] = x['attention_mask'][selected_ids.nonzero(as_tuple=True)]\n",
    "# print(f'{x[\"input_ids\"].shape=}')\n",
    "# print(f'{x[\"attention_mask\"].shape=}')\n",
    "# # x_decoded = tokenizer.batch_decode(x['input_ids'],\n",
    "# #                                   skip_special_tokens=True)\n",
    "# # print(x_decoded[0])\n",
    "# print(f'-------------------------------------------------')\n",
    "\n",
    "# ### MAX-TOKEN-LENGTH THRESHOLDING\n",
    "# print(f'MAX-TOKEN-LENGTH THRESHOLDING:')\n",
    "# numtokens = get_lengths(x['attention_mask'])\n",
    "# Ns,max_length_in_batch = x[\"input_ids\"].shape\n",
    "# numpads = max_length_in_batch - numtokens\n",
    "# z = torch.empty(size=(Ns,max_length),dtype=torch.int)\n",
    "# for i in range(Ns):\n",
    "#   z[i] = x[\"input_ids\"][i,numpads[i]:numpads[i]+max_length]\n",
    "# print(f'{z.shape=}')\n",
    "# print(f'{z=}')\n",
    "# print(f'-------------------------------------------------')\n",
    "\n",
    "# ### REMOVING REPETITIONS\n",
    "# print(f'REMOVING REPETITIONS:')\n",
    "# z = torch.unique(z,dim=0) # this reorders the sentences\n",
    "# print(f'{z.shape=}')\n",
    "# Ns,_ = z.shape\n",
    "# assert _ == max_length\n",
    "# print(f'-------------------------------------------------')\n",
    "\n",
    "# ### RANDOMIZING\n",
    "# if randomize:\n",
    "#   print('RANDOMIZING:')\n",
    "#   if LLM == 'OPT':\n",
    "#     z = z[:,1:]\n",
    "#     z = z.reshape(Ns*(max_length-1))[torch.randperm(Ns*(max_length-1))]\n",
    "#     z = z.reshape(Ns,(max_length-1))\n",
    "#     z = torch.cat((2*torch.ones(size=(Ns,1),dtype=int),z),\n",
    "#                           dim=-1)\n",
    "#   elif LLM == 'Pythia':\n",
    "#     z = z.reshape(Ns*(max_length))[torch.randperm(Ns*(max_length))]\n",
    "#     z = z.reshape(Ns,(max_length))\n",
    "#   print(f'{z=}')\n",
    "# np.savetxt(tokens_outputfolder + f'token_ids.txt',z,fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
